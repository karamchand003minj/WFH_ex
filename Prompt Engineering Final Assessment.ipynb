{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72791ca2-cdde-49bb-95aa-7a4e509a0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b05ad7-f715-4d88-b9ab-4c51c622f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59982b8-e523-4643-8769-fd67f6ed9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        out, h = self.rnn(x, h)\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524e5f3f-e32f-4bbe-8755-f21c878a2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        hidden = None\n",
    "        for x, y in data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(x, hidden)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f72032d-5c41-46ea-86d8-82da41797fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, num_words):\n",
    "    model.eval()\n",
    "    text = seed_text\n",
    "    for _ in range(num_words):\n",
    "        x = torch.tensor([text[-1]])\n",
    "        output, _ = model(x, None)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        text.append(predicted.item())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20de34c4-2788-421c-9e7b-296ada447892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karamchand Minj\\anaconda3\\envs\\prompt_engineering\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4606ac-cd13-47b3-ba2d-78fe6376b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(prompt):\n",
    "    return tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "def decode(encoded_prompt):\n",
    "    return tokenizer.decode(encoded_prompt[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8be14ba-e9b9-456b-8b88-ad7fb0929590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm an AI model. Hello, Tell me about our solar system? why we not consider pluto in our solar system. How can we make an Earth to planets with planets. What is our current orbit based on? Where do we need to call the time for your solar cycle?\n",
      "\n",
      "What is your planet's orbit base? What are the positions you can be in this situation? A planet is defined by its orbital position, the altitude above its body, its\n"
     ]
    }
   ],
   "source": [
    "# conversation history\n",
    "history_encoded = tokenizer.encode(\"Hello, I'm an AI model. \", return_tensors=\"pt\")\n",
    "\n",
    "# user input\n",
    "user_input_encoded = tokenizer.encode(\"Hello, Tell me about our solar system? why we not consider pluto in our solar system. \", return_tensors=\"pt\")\n",
    "\n",
    "# append the new user input tokens to the chat history\n",
    "history_with_user_input_encoded = torch.cat([history_encoded, user_input_encoded], dim=-1)\n",
    "\n",
    "# generate a response\n",
    "output = model.generate(history_with_user_input_encoded, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, do_sample=True)\n",
    "\n",
    "history_with_reply_encoded = output\n",
    "\n",
    "# Print message\n",
    "output_message = decode(history_with_reply_encoded)\n",
    "print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e30339a-7be1-4b85-b262-980e6e1a5097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]\n",
      "C:\\Users\\Karamchand Minj\\anaconda3\\envs\\prompt_engineering\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Karamchand Minj\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 483/483 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.39MB/s]\n",
      "tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 654kB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 268M/268M [01:48<00:00, 2.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query 1: What is the weather forecast for tomorrow?\n",
      "Generated Prompt 1: The user asked: [CLS] what is the weather forecast for tomorrow? [SEP]\n",
      "\n",
      "Sample Query 2: Can you tell me the latest news headlines?\n",
      "Generated Prompt 2: The user asked: [CLS] can you tell me the latest news headlines? [SEP]\n",
      "\n",
      "Sample Query 3: How does climate change affect wildlife?\n",
      "Generated Prompt 3: The user asked: [CLS] how does climate change affect wildlife? [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 1. Load the transformer model's encoder from the library package.\n",
    "encoder_model_name = \"distilbert-base-uncased\"  # Example model, you can choose any other model\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n",
    "encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "\n",
    "# 2. Prepare a list of sample user queries related to a specific topic.\n",
    "sample_queries = [\n",
    "    \"What is the weather forecast for tomorrow?\",\n",
    "    \"Can you tell me the latest news headlines?\",\n",
    "    \"How does climate change affect wildlife?\",\n",
    "]\n",
    "\n",
    "# 3. Encode each query using the encoder model.\n",
    "encoded_queries = []\n",
    "for query in sample_queries:\n",
    "    encoded_query = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    encoded_queries.append(encoded_query)\n",
    "\n",
    "# 4. Generate prompts for the AI to respond to based on the encoded queries.\n",
    "generated_prompts = []\n",
    "for encoded_query in encoded_queries:\n",
    "    generated_prompt = \"The user asked: \" + tokenizer.decode(encoded_query.input_ids[0])\n",
    "    generated_prompts.append(generated_prompt)\n",
    "\n",
    "# 5. Display the prompts and the corresponding encoded queries for verification.\n",
    "for i, (query, prompt) in enumerate(zip(sample_queries, generated_prompts)):\n",
    "    print(f\"Sample Query {i+1}: {query}\")\n",
    "    print(f\"Generated Prompt {i+1}: {prompt}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00697c2-d3e9-4e78-a661-5032bcac02d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_engineering",
   "language": "python",
   "name": "prompt_engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
